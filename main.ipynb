{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Constants and Game Class (Unchanged from Phase 1) ---\n",
    "ROWS = 6\n",
    "COLS = 7\n",
    "PLAYER_PIECE = 1\n",
    "AI_PIECE = 2\n",
    "PLAYER_EMOJI = 'ðŸ”µ'\n",
    "AI_EMOJI = 'ðŸ”´'\n",
    "EMPTY_EMOJI = 'âš«'\n",
    "\n",
    "class Connect4Game:\n",
    "    # (The Connect4Game class code from the previous step goes here)\n",
    "    # (No changes are needed for the class itself)\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
    "        self.game_over = False\n",
    "        self.turn = 0 # 0 for Player, 1 for AI\n",
    "\n",
    "    def drop_piece(self, row, col, piece):\n",
    "        self.board[row][col] = piece\n",
    "\n",
    "    def is_valid_location(self, col):\n",
    "        return self.board[ROWS-1][col] == 0\n",
    "\n",
    "    def get_next_open_row(self, col):\n",
    "        for r in range(ROWS):\n",
    "            if self.board[r][col] == 0:\n",
    "                return r\n",
    "\n",
    "    def print_board(self):\n",
    "        flipped_board = np.flip(self.board, 0)\n",
    "        print(\"  \".join(map(str, range(COLS))))\n",
    "        print(\"-\" * (COLS * 3))\n",
    "        for r in range(ROWS):\n",
    "            row_str = \"\"\n",
    "            for c in range(COLS):\n",
    "                if flipped_board[r][c] == PLAYER_PIECE:\n",
    "                    row_str += PLAYER_EMOJI + \" \"\n",
    "                elif flipped_board[r][c] == AI_PIECE:\n",
    "                    row_str += AI_EMOJI + \" \"\n",
    "                else:\n",
    "                    row_str += EMPTY_EMOJI + \" \"\n",
    "            print(row_str)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def winning_move(self, piece):\n",
    "        # Check all winning conditions (horizontal, vertical, diagonals)\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(ROWS):\n",
    "                if all(self.board[r, c+i] == piece for i in range(4)): return True\n",
    "        for c in range(COLS):\n",
    "            for r in range(ROWS - 3):\n",
    "                if all(self.board[r+i, c] == piece for i in range(4)): return True\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(ROWS - 3):\n",
    "                if all(self.board[r+i, c+i] == piece for i in range(4)): return True\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(3, ROWS):\n",
    "                if all(self.board[r-i, c+i] == piece for i in range(4)): return True\n",
    "        return False\n",
    "\n",
    "    def get_valid_locations(self):\n",
    "        return [col for col in range(COLS) if self.is_valid_location(col)]\n",
    "\n",
    "# --- Heuristic Bot Logic (Unchanged, our agent's opponent) ---\n",
    "def heuristic_move(board):\n",
    "    valid_locations = [c for c in range(COLS) if board[ROWS-1][c] == 0]\n",
    "    for col in valid_locations:\n",
    "        temp_board = board.copy()\n",
    "        row = get_next_open_row_for_board(temp_board, col)\n",
    "        temp_board[row][col] = AI_PIECE\n",
    "        if winning_move_for_board(temp_board, AI_PIECE): return col\n",
    "    for col in valid_locations:\n",
    "        temp_board = board.copy()\n",
    "        row = get_next_open_row_for_board(temp_board, col)\n",
    "        temp_board[row][col] = PLAYER_PIECE\n",
    "        if winning_move_for_board(temp_board, PLAYER_PIECE): return col\n",
    "    return random.choice(valid_locations) if valid_locations else None\n",
    "\n",
    "def get_next_open_row_for_board(board, col):\n",
    "    for r in range(ROWS):\n",
    "        if board[r, col] == 0: return r\n",
    "\n",
    "def winning_move_for_board(board, piece):\n",
    "    for c in range(COLS - 3):\n",
    "        for r in range(ROWS):\n",
    "            if all(board[r, c+i] == piece for i in range(4)): return True\n",
    "    for c in range(COLS):\n",
    "        for r in range(ROWS - 3):\n",
    "            if all(board[r+i, c] == piece for i in range(4)): return True\n",
    "    for c in range(COLS - 3):\n",
    "        for r in range(ROWS - 3):\n",
    "            if all(board[r+i, c+i] == piece for i in range(4)): return True\n",
    "    for c in range(COLS - 3):\n",
    "        for r in range(3, ROWS):\n",
    "            if all(board[r-i, c+i] == piece for i in range(4)): return True\n",
    "    return False\n",
    "\n",
    "# --- NEW: Q-Learning Agent Class ---\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.9999, epsilon_min=0.01):\n",
    "        # Use defaultdict to handle unseen states gracefully\n",
    "        self.q_table = defaultdict(lambda: np.zeros(COLS))\n",
    "        self.alpha = alpha         # Learning rate\n",
    "        self.gamma = gamma         # Discount factor for future rewards\n",
    "        self.epsilon = epsilon     # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def get_state_key(self, board):\n",
    "        # Convert the numpy array board state into a hashable type (bytes)\n",
    "        return board.tobytes()\n",
    "\n",
    "    def choose_action(self, board, valid_locations):\n",
    "        # Epsilon-greedy strategy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_locations) # Explore\n",
    "        else:\n",
    "            state_key = self.get_state_key(board)\n",
    "            q_values = self.q_table[state_key]\n",
    "            # Choose the best action among valid locations\n",
    "            valid_q_values = [q_values[col] for col in valid_locations]\n",
    "            return valid_locations[np.argmax(valid_q_values)] # Exploit\n",
    "\n",
    "    def update_q_table(self, board_state, action, reward, next_board_state):\n",
    "        state_key = self.get_state_key(board_state)\n",
    "        next_state_key = self.get_state_key(next_board_state)\n",
    "\n",
    "        old_value = self.q_table[state_key][action]\n",
    "        next_max = np.max(self.q_table[next_state_key]) # Future reward\n",
    "\n",
    "        # The Q-learning formula\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[state_key][action] = new_value\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_q_table(self, filename=\"q_table.pkl\"):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "            print(f\"Q-table saved to {filename}\")\n",
    "\n",
    "    def load_q_table(self, filename=\"q_table.pkl\"):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.q_table = defaultdict(lambda: np.zeros(COLS), pickle.load(f))\n",
    "            print(f\"Q-table loaded from {filename}\")\n",
    "\n",
    "\n",
    "# --- NEW: Training Loop ---\n",
    "agent = QLearningAgent()\n",
    "episodes = 100000 # The number of games the AI will play to learn\n",
    "wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "print(\"Starting Q-learning training...\")\n",
    "\n",
    "for episode in range(episodes):\n",
    "    game = Connect4Game()\n",
    "    # Let the Q-learning agent be Player 1\n",
    "\n",
    "    while not game.game_over:\n",
    "        if game.turn == 0: # Agent's turn\n",
    "            state = game.board.copy()\n",
    "            valid_locations = game.get_valid_locations()\n",
    "            if not valid_locations: break\n",
    "\n",
    "            col = agent.choose_action(state, valid_locations)\n",
    "\n",
    "            row = game.get_next_open_row(col)\n",
    "            game.drop_piece(row, col, PLAYER_PIECE)\n",
    "\n",
    "            if game.winning_move(PLAYER_PIECE):\n",
    "                reward = 20 # Big reward for winning\n",
    "                agent.update_q_table(state, col, reward, game.board)\n",
    "                wins += 1\n",
    "                game.game_over = True\n",
    "\n",
    "            game.turn += 1\n",
    "\n",
    "        else: # Opponent's (Heuristic Bot) turn\n",
    "            opponent_col = heuristic_move(game.board)\n",
    "            if opponent_col is not None:\n",
    "                row = game.get_next_open_row(opponent_col)\n",
    "                game.drop_piece(row, opponent_col, AI_PIECE)\n",
    "\n",
    "                if game.winning_move(AI_PIECE):\n",
    "                    reward = -20 # Big penalty for losing\n",
    "                    agent.update_q_table(state, col, reward, game.board)\n",
    "                    losses += 1\n",
    "                    game.game_over = True\n",
    "\n",
    "            # If the game isn't over, give a small negative reward to encourage faster wins\n",
    "            if not game.game_over:\n",
    "                reward = -0.5\n",
    "                agent.update_q_table(state, col, reward, game.board)\n",
    "\n",
    "            game.turn -= 1\n",
    "\n",
    "        # Check for draw\n",
    "        if len(game.get_valid_locations()) == 0 and not game.game_over:\n",
    "            reward = 5 # Small reward for a draw\n",
    "            agent.update_q_table(state, col, reward, game.board)\n",
    "            draws +=1\n",
    "            game.game_over = True\n",
    "\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {episode + 1}/{episodes}\")\n",
    "        win_rate = (wins / (episode + 1)) * 100\n",
    "        print(f\"Win Rate: {win_rate:.2f}% ({wins} W / {losses} L / {draws} D)\")\n",
    "        print(f\"Current Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "agent.save_q_table() # Save the learned strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- We need to include the class definitions again in this new cell ---\n",
    "\n",
    "# --- Constants and Game Class ---\n",
    "ROWS = 6\n",
    "COLS = 7\n",
    "PLAYER_PIECE = 1\n",
    "AI_PIECE = 2\n",
    "PLAYER_EMOJI = 'ðŸ”µ' # You are Player 1\n",
    "AI_EMOJI = 'ðŸ”´'   # The Q-learning agent is Player 2\n",
    "EMPTY_EMOJI = 'âš«'\n",
    "\n",
    "class Connect4Game:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
    "        self.game_over = False\n",
    "        self.turn = 0 # 0 for Player, 1 for AI\n",
    "\n",
    "    def drop_piece(self, row, col, piece):\n",
    "        self.board[row][col] = piece\n",
    "\n",
    "    def is_valid_location(self, col):\n",
    "        return self.board[ROWS-1][col] == 0\n",
    "\n",
    "    def get_next_open_row(self, col):\n",
    "        for r in range(ROWS):\n",
    "            if self.board[r][col] == 0:\n",
    "                return r\n",
    "\n",
    "    def print_board(self):\n",
    "        flipped_board = np.flip(self.board, 0)\n",
    "        print(\"  \".join(map(str, range(COLS))))\n",
    "        print(\"-\" * (COLS * 3))\n",
    "        for r in range(ROWS):\n",
    "            row_str = \"\"\n",
    "            for c in range(COLS):\n",
    "                if flipped_board[r][c] == PLAYER_PIECE:\n",
    "                    row_str += PLAYER_EMOJI + \" \"\n",
    "                elif flipped_board[r][c] == AI_PIECE:\n",
    "                    row_str += AI_EMOJI + \" \"\n",
    "                else:\n",
    "                    row_str += EMPTY_EMOJI + \" \"\n",
    "            print(row_str)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def winning_move(self, piece):\n",
    "        # Check all winning conditions\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(ROWS):\n",
    "                if all(self.board[r, c+i] == piece for i in range(4)): return True\n",
    "        for c in range(COLS):\n",
    "            for r in range(ROWS - 3):\n",
    "                if all(self.board[r+i, c] == piece for i in range(4)): return True\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(ROWS - 3):\n",
    "                if all(self.board[r+i, c+i] == piece for i in range(4)): return True\n",
    "        for c in range(COLS - 3):\n",
    "            for r in range(3, ROWS):\n",
    "                if all(self.board[r-i, c+i] == piece for i in range(4)): return True\n",
    "        return False\n",
    "\n",
    "    def get_valid_locations(self):\n",
    "        return [col for col in range(COLS) if self.is_valid_location(col)]\n",
    "\n",
    "# --- Q-Learning Agent Class ---\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(COLS))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_state_key(self, board):\n",
    "        return board.tobytes()\n",
    "\n",
    "    def choose_action(self, board, valid_locations):\n",
    "        # Epsilon-greedy strategy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_locations) # Explore\n",
    "        else:\n",
    "            state_key = self.get_state_key(board)\n",
    "            q_values = self.q_table[state_key]\n",
    "            valid_q_values = {col: q_values[col] for col in valid_locations}\n",
    "            return max(valid_q_values, key=valid_q_values.get) # Exploit\n",
    "\n",
    "    def load_q_table(self, filename=\"q_table.pkl\"):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.q_table = defaultdict(lambda: np.zeros(COLS), pickle.load(f))\n",
    "                print(f\"Q-table loaded successfully from {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find {filename}. The agent has not been trained.\")\n",
    "            # Exit or handle the error as needed\n",
    "            raise\n",
    "\n",
    "# --- NEW: Interactive Playing Loop ---\n",
    "\n",
    "# 1. Initialize the agent and the game\n",
    "agent = QLearningAgent()\n",
    "game = Connect4Game()\n",
    "\n",
    "# 2. Load the trained \"brain\"\n",
    "agent.load_q_table(\"q_table.pkl\")\n",
    "agent.epsilon = 0 # IMPORTANT: Set epsilon to 0 to always use the best strategy\n",
    "\n",
    "# You are Player 1 (Blue), the AI is Player 2 (Red)\n",
    "game.turn = 0 # Player starts\n",
    "\n",
    "clear_output()\n",
    "game.print_board()\n",
    "\n",
    "while not game.game_over:\n",
    "    if game.turn == 0: # Your turn\n",
    "        try:\n",
    "            col = int(input(f\"Your move, Player {PLAYER_EMOJI} (0-6): \"))\n",
    "            if 0 <= col < COLS and game.is_valid_location(col):\n",
    "                row = game.get_next_open_row(col)\n",
    "                game.drop_piece(row, col, PLAYER_PIECE)\n",
    "\n",
    "                if game.winning_move(PLAYER_PIECE):\n",
    "                    print(\"CONGRATULATIONS! YOU WIN!\")\n",
    "                    game.game_over = True\n",
    "\n",
    "                game.turn += 1 # Switch to AI's turn\n",
    "            else:\n",
    "                print(\"Invalid column. Please try again.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    else: # Agent's turn\n",
    "        print(\"AI is thinking...\")\n",
    "        time.sleep(1) # Add a small delay to simulate thinking\n",
    "\n",
    "        valid_locations = game.get_valid_locations()\n",
    "        if not valid_locations:\n",
    "            break\n",
    "\n",
    "        col = agent.choose_action(game.board, valid_locations)\n",
    "        row = game.get_next_open_row(col)\n",
    "        game.drop_piece(row, col, AI_PIECE)\n",
    "\n",
    "        if game.winning_move(AI_PIECE):\n",
    "            print(\"THE Q-LEARNING AGENT WINS!\")\n",
    "            game.game_over = True\n",
    "\n",
    "        game.turn -= 1 # Switch to your turn\n",
    "\n",
    "    # Check for a draw\n",
    "    if len(game.get_valid_locations()) == 0 and not game.game_over:\n",
    "        print(\"IT'S A DRAW!\")\n",
    "        game.game_over = True\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    game.print_board()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
